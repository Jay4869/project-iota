match(2, pred$F)
which(pred$F == 2)
prob = 1/length(which(pred$Freq == max(pred$Freq)))
prob
?sample
sample(c('r', 'b'), 1, prob = 0.5)
sample(c('r', 'b'), 1, prob = c(0.5,0.5))
sample(c('r', 'b'), 1, prob = c(0.5,0.5))
sample(c('r', 'b'), 1, prob = c(0.5,0.5))
sample(c('r', 'b'), 1, prob = c(0.5,0.5))
sample(c('r', 'b'), 1, prob = c(2,2))
sample(c('r', 'b'), 1, prob = c(2,2))
sample(c('r', 'b'), 1, prob = c(2,2))
sample(c('r', 'b'), 1, prob = c(2,2))
prob = rep(length(which(pred$Freq == max(pred$Freq))), length(which(pred$Freq == max(pred$Freq))))
prob
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob))
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
k1
K = 1
K = 3
result = y[match(sort(dist1, TRUE)[1:K], dist1)]
pred = sort(table(result),TRUE)[1]
pred
pred = as.data.frame(table(a))
prob = rep(length(which(pred$Freq == max(pred$Freq))), length(which(pred$Freq == max(pred$Freq))))
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
prob
pred
result = y[match(sort(dist1, TRUE)[1:K], dist1)]
pred = sort(table(result),TRUE)[1]
pred
pred = as.data.frame(table(result))
prob = rep(length(which(pred$Freq == max(pred$Freq))), length(which(pred$Freq == max(pred$Freq))))
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
pred = as.data.frame(table(result))
prob = rep(length(which(pred$Freq == max(pred$Freq))), length(which(pred$Freq == max(pred$Freq))))
sample(pred$a[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
sample(pred$result[which(pred$Freq == max(pred$Freq))], 1, prob = prob)
prob
pred
a
pred = as.data.frame(table(a))
pred
if !require("MASS")
if (!require("MASS"))
{install.packages("MASS")}
library("boot")
library(caret)
if(!require("caret"))
{
install.packages("caret")
}
if(!require("MASS")){install.packages("MASS")}
if(!require("boot")){install.packages("boot")}
head(data)
names(data)
if(!require("glment")){install.packages("glmnet")}
library(glmnet)
cv.out = cv.glmnet(x.train, y.train, alpha = 0)
cv.out
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
paste("best lamda of ridge is: ", bestlam)
if(!require("randomForest")){install.packages("randomForest")}
library(randomForest)
library(MASS)
if(!require("MASS")){install.packages("MASS")}
library(Boston)
data = data(Boston)
head(data)
View(data)
data(Boston)
Boston
View(Boston)
data = Boston
data = na.omit(data)
names(data)
?Boston
dim(Boston)
names(data)
names(data)
set.seed(123)
index = sample(nrow(data), 0.8 * nrow(data))
train = data[index,]
test = data[-index,]
head(test)
x = data[,-1]
head(x)
head(data)
x = data[,-14]
head(x)
y = data[, 14]
head(y)
x.train = train[, -14]
y.train = train[, 14]
x.test = test[,-14]
y.test = test[, 14]
head(x.train)
dim(x.train)
dim(train)
dim(ata)
dim(data)
404/506
pairs(data)
summary(data)
if(!require("glment")){install.packages("glmnet")}
library(glmnet)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 5)
cv.out
plot(cv.out)
rm(cv.out)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 5)
cv.out = cv.glmnet(x.train, y.train, alpha = 0)
y.train
data(Boston)
data = Boston
data = na.omit(data)
names(data)
```
Here I give the basic statistical summary of the Boston dataset which shows the mean, meidan, standard deviation, min, and max. In the summary, I realized that some predictors vary observation such as `crim`, `zn`, `tax`. These have the low mean, but contains high maximent, which might happen different group dataset or outliers.
```{r, echo=FALSE}
summary(data)
pairs(data)
set.seed(123)
index = sample(nrow(data), 0.8 * nrow(data))
train = data[index,]
test = data[-index,]
x = data[,-14]
y = data[, 14]
x.train = train[, -14]
y.train = train[, 14]
x.test = test[,-14]
y.test = test[, 14]
library(glmnet)
cv.out = cv.glmnet(x.train, y.train, alpha = 0, nfolds = 5)
typeof(x.train)
typeof(y.train)
as.vector(y.train)
typeof(as.vector(y.train))
typeof(train$medv)
cv.out = cv.glmnet(as.matrix(x.train), y.train, alpha = 0, nfolds = 5)
plot(cv.out)
bestlam = cv.out$lambda.min
paste("best lamda of ridge is: ", bestlam)
ridge.pred = predict(ridge.model, s = bestlam, newx = as.matrix(x.test))
ridge.model = glmnet(as.matrix(x.train), y.train, alpha = 0)
ridge.pred = predict(ridge.model, s = bestlam, newx = as.matrix(x.test))
paste("MSE of Ridge regression: ", mean((ridge.pred-y.test)^2))
ncol(x.train)
if(!require("randomForest")){install.packages("randomForest")}
library(randomForest)
head(train)
medv
dim(y.train)
dim(train)
len(y.train)
length(y.train)
bag.tree=randomForest(y.train~., data=train, mtry=ncol(x.train), ntree=1000, importance=TRUE)
plot(bag.tree)
bag.tree=randomForest(y.train~., data=train, mtry=ncol(x.train), ntree=500, importance=TRUE)
plot(bag.tree)
bag.pred = predict(bag.tree, newdata=test)
paste("MSE of bagging tree: ", mean((y.test - bag.pred)^2))
bagging.cv(bag.tree, v=5)
if(!require("adabag")){install.packages("adabag")}
library(adabag)
bagging.cv(bag.tree, v=5)
bagging.cv(bag.tree, data = train, v=5)
bagging.cv(bag.tree, data = as.dataframe(train), v=5)
bagging.cv(bag.tree, data = x.train, v=5)
bagging.cv(y.train~., data = train, v=5)
train
typeof(train)
as.dataframe(typeof(train))
bagging.cv(factor(y.train)~., data = train, v=5)
bagging.cv(y.train~., data = x.train, v=5)
bagging.cv(y.train~., data = as.data.frame(x.train), v=5)
bag.pred = predict(bag.tree, newdata=test)
paste("MSE of bagging tree: ", mean((y.test - bag.pred)^2))
sub.train = sample(nrow(x.train), 0.5*nrow(x.train))
x.train.sub = x.train[sub.train, ]
dim(x.train.sub)
y.train.sub = y.train[sub.train]
x.test.sub = x.test[sub.train,]
x.test.sub = x.test[-sub.train,]
y.test.sub = y.test[-sub.train]
lam.grip = c(0.1, 0.01, 0.001)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0)
library(glmnet)
lam.grip = c(0.1, 0.01, 0.001)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0)
ridge.model.sub
ridge.pred.sub = predict(ridge.model.sub, s = lam.grip, newx = as.matrix(x.test.sub))
ridge.model.sub
ridge.pred.sub = predict(ridge.model.sub, s = 0.1, newx = as.matrix(x.test.sub))
ridge.model.sub
lam.grip = 10^seq(10, -2, length = 10)
lam.grip
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0, lambda=lam.grip, thresh=1e-12)
ridge.model.sub
ridge.pred.sub$%Dev
summary(ridge.pred.sub)
names(ridge.pred.sub)
lam.grip = c(1,2,3,4,5)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0, lambda=lam.grip, thresh=1e-12)
redge.model.sub
ridge.model.sub
lam.grip = 10^seq(10, -2, length = 10)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0, lambda=lam.grip, thresh=1e-12)
for(i in 1:10)
{}
ridge.model.sub
ridge.pred.sub = predict(ridge.model.sub, s = 2.154e+07, newx = as.matrix(x.test.sub))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
ridge.pred.sub = predict(ridge.model.sub, s = 1.000e+10, newx = as.matrix(x.test.sub))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
for(i in 1:length(lam.grip))
{
ridge.pred.sub = predict(ridge.model.sub, s = lam.grip[i], newx = as.matrix(x.test.sub))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
}
for(i in 1:length(lam.grip))
{
ridge.pred.sub = predict(ridge.model.sub, s = lam.grip[i], newx = as.matrix(x.test.sub))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
}
bag.tree=randomForest(y.train~., data=train, mtry=ncol(x.train), ntree=500, importance=TRUE)
plot(bag.tree)
#bagging.cv(y.train~., data =train, v=5)
bag.pred = predict(bag.tree, newdata=test)
paste("MSE of bagging tree: ", mean((y.test - bag.pred)^2))
library(glmnet)
cv.out = cv.glmnet(as.matrix(x.train), y.train, alpha = 0, nfolds = 5)
plot(cv.out)
bestlam = cv.out$lambda.min
paste("best lamda of ridge is: ", bestlam)
ridge.model = glmnet(as.matrix(x.train), y.train, alpha = 0)
ridge.pred = predict(ridge.model, s = bestlam, newx = as.matrix(x.test))
paste("MSE of Ridge regression: ", mean((ridge.pred-y.test)^2))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
lam.grip = 10^seq(10, -2, length = 10)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0, lambda=lam.grip, thresh=1e-12)
reidge.model.sub
ridge.pred.sub = predict(ridge.model.sub, s = 1.000e+10, newx = as.matrix(x.test.sub))
paste("MSE of Ridge regression: ", mean((ridge.pred.sub-y.test.sub)^2))
ridge.pred = predict(ridge.model, s = 1.000e+10, newx = as.matrix(x.test))
ridge.pred = predict(ridge.model, s = 1.000e+10, newx = as.matrix(x.test))
paste("MSE of Ridge regression from test set: ", mean((ridge.pred.sub-y.test.sub)^2))
ridge.coef = predict(out, type = "coefficients", s = bestlam)
out = glmnet(x, y, alpha = 0)
out = glmnet(as.matrix(x), y, alpha = 0)
ridge.coef = predict(out, type = "coefficients", s = bestlam)
ridge.coef
bag.tree=randomForest(y.train~., data=train, mtry=ncol(x.train), ntree=500, importance=TRUE)
plot(bag.tree)
#bagging.cv(y.train~., data =train, v=5)
bag.pred = predict(bag.tree, newdata=test)
paste("MSE of bagging tree: ", mean((y.test - bag.pred)^2))
bag.tree=randomForest(y~., data=data, mtry=ncol(x.train), ntree=500, importance=TRUE)
plot(bag.tree)
bag.pred = predict(bag.tree, newdata=test)
if(!require("randomForest")){install.packages("randomForest")}
if(!require("adabag")){install.packages("adabag")}
library(randomForest)
library(adabag)
bag.tree=randomForest(y.train~., data=train, mtry=ncol(x.train), ntree=500, importance=TRUE)
plot(bag.tree)
bag.pred = predict(bag.tree, newdata=test)
paste("MSE of bagging tree: ", mean((y.test - bag.pred)^2))
```
Applying the 5-fold cross validaiton on Ridge regression model with training set to find the optimal lambda
```{r, echo=FALSE}
#ridge
if(!require("glment")){install.packages("glmnet")}
library(glmnet)
cv.out = cv.glmnet(as.matrix(x.train), y.train, alpha = 0, nfolds = 5)
plot(cv.out)
bestlam = cv.out$lambda.min
paste("best lamda of ridge is: ", bestlam)
```
Based on best lamda from 5-fold-cv, I calculate the MSE based on testing set.
```{r,echo=FALSE}
ridge.model = glmnet(as.matrix(x.train), y.train, alpha = 0)
ridge.pred = predict(ridge.model, s = bestlam, newx = as.matrix(x.test))
paste("MSE of Ridge regression: ", mean((ridge.pred-y.test)^2))
```
Using validation set approach to select the model. I splite the train set to a small training set and a validation set half by half. I found the best lambda is 10 and I fit in test set to get MSE.
```{r, echo=FALSE}
library(glmnet)
sub.train = sample(nrow(x.train), 0.5*nrow(x.train))
x.train.sub = x.train[sub.train, ]
y.train.sub = y.train[sub.train]
x.test.sub = x.test[-sub.train,]
y.test.sub = y.test[-sub.train]
lam.grip = 10^seq(10, -2, length = 10)
ridge.model.sub = glmnet(as.matrix(x.train.sub), y.train.sub, alpha = 0, lambda=lam.grip, thresh=1e-12)
reidge.model.sub
ridge.pred.sub = predict(ridge.model.sub, s = 1.000e+10, newx = as.matrix(x.test.sub))
ridge.pred = predict(ridge.model, s = 1.000e+10, newx = as.matrix(x.test))
paste("MSE of Ridge regression from test set: ", mean((ridge.pred.sub-y.test.sub)^2))
```
ridge.pred.sub = predict(ridge.model.sub, s = 1.000e+10, newx = as.matrix(x.test.sub))
ridge.pred = predict(ridge.model, s = 1.000e+10, newx = as.matrix(x.test))
paste("MSE of Ridge regression from test set: ", mean((ridge.pred.sub-y.test.sub)^2))
```
#4. Results
Based on all the MSE of testing set from each method, I find the smallest MSE is ridge.coef. Therefor, I would like to apply bagging on the whole dataset and produce the coefficient.
```{r,echo=FALSE}
out = glmnet(as.matrix(x), y, alpha = 0)
ridge.coef = predict(out, type = "coefficients", s = bestlam)
ridge.coef
```
if(!require("MASS")){install.packages("MASS")}
Boston
class(Boston)
typeof(Boston)
library(adabag)
install.packages(rpart)
install.packages("rpart")
install.packages("rpart")
install.packages(c("mlbench","caret","lattice","ggplot2")
)
install.packages(c("mlbench", "caret", "lattice", "ggplot2"))
install.packages(c("mlbench", "caret", "lattice", "ggplot2"))
install.packages(c("mlbench", "caret", "lattice", "ggplot2"))
library(adabag)
install.packages("adabag")
library(adabag)
library(rpart)
library(mlbench)
library(caret)
install.packages("caret")
install.packages("caret")
library("caret")
library("lattice")
library("ggplot2")
library("rpart")
library("mlbench")
library("adabag")
out = bagging.cv(Boston$medv~., data = Boston, v=10)
library(rpart)
data(iris)
iris.baggingcv <- bagging.cv(Species ~ ., v=10, data=iris, mfinal=10,control=rpart.control(maxdepth=3))
out = bagging.cv(Boston$medv~., v=10, data=Boston, mfinal=10, control=rpart.control(maxdepth=2))
sample(c(0,1), 10, prob=c(0.6,0.4))
sample(c(0,1), 10, replace = T, prob=c(0.6,0.4))
sample(c(0,1), 10, replacement = T, prob=c(0.6,0.4))
sample(c(0,1), 10, replacement = T, prob=c(0.6,0.4))
?sample
sample(c(0,1), 10, reokace = T, prob=c(0.6,0.4))
sample(c(0,1), 10, reokace = TRUE, prob=c(0.6,0.4))
sample(c(0,1), 10, replace = TRUE, prob=c(0.6,0.4))
for (i in 1:10)
{
sampel(c(0,1), prob = c(0.6,0.4))
}
for (i in 1:10)
{
sample(c(0,1), prob = c(0.6,0.4))
}
for (i in 1:10)
{
print(sample(c(0,1), prob=c(0.6,0.4)))}
sample(1:6, 1)
sample(1:6, 10, replace=T)
sample(1:6, 10, replace=TRUE)
t = rep(NA, 100)
x = sample(1:6, 10, replace=TRUE)
sum(x == 6)
x
sum(x == 6) > sum(x ==4)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
t
mean(t)
sample(c(0,1), 1, replace=TRUE, prob=(1/3, 2/3))
sample(c(0,1), 1, replace=TRUE, prob=(0.33,0.67))
sample(c(0,1), 1, replace=TRUE, prob=(1/3, 2/3))
sample(c(0,1), 1, replace=TRUE, prob=(1/3, 2/3))
sample(c(0,1), 1, replace=TRUE, prob=c(1/3, 2/3))
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(c(0,1), 5, replace=TRUE, prob=c(1/3, 2/3))
t[i] = x[1] == x[5]
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
t = rep(NA, 100)
for (i in 1:100)
{
x = sample(1:6, 10, replace=TRUE)
t[i] = sum(x == 6) > sum(x == 4)
}
mean(t)
rbinom(1,4,c(2/3, 1/3))
flips = rbinom(1000, 4, c(2/3,1/3))
mean(flips%%2)
t = rep(NA, 1000000)
for (i in 1:1000000)
{
x = sample(c(0,1), 5, replace=TRUE, prob=c(1/3, 2/3))
t[i] = x[1] == x[5]
}
mean(t)
t = rep(NA, 1000)
for (i in 1:1000)
{
x = sample(c(0,1), 5, replace=TRUE, prob=c(1/3, 2/3))
t[i] = x[1] == x[5]
}
mean(t)
rbinom(10, 1, prob=c(2/3,1/3))
mean(rbinom(10, 1, prob=c(2/3,1/3)))
mean(rbinom(10, 1, prob=c(2/3,1/3)))
mean(rbinom(1000, 1, prob=c(2/3,1/3)))
mean(rbinom(1000, 1, prob=c(2/3,1/3)))
mean(rbinom(1000, 1, prob=c(2/3,1/3)))
setwd('E:/GitHub/project-iota/data/beta')
beta = read.table('task001_run001_conv005.txt')
p = read.table('task001_run001_conv005_p-value.txt')
which.min(p)
p
View(p)
which.min(p)
p = as.vector(p)
which.min(p)
p
which.min(p$V1)
data = read.table('../convo/task001_run001_conv005.txt')
length(data)
data = read.txt('../convo/task001_run001_conv005.txt')
dim(data)
data = read.table('../convo/task001_run001_conv005.txt')[,4:]
data = read.table('../convo/task001_run001_conv005.txt')[,4:nrow(data)]
data = read.table('../convo/task001_run001_conv005.txt')[5:nrow(data),]
dim(data)
dim(data)
data = read.table('../convo/task001_run001_conv005.txt')[5:nrow(data),]
nrow(data)
data = read.table('../convo/task001_run001_conv005.txt')[5:dim(data)[1],]
data[1,1]
data = read.table('../convo/task001_run001_conv005.txt')
View(data)
data = data$V1
data
data[5]
data = data$V1[5:length(data)]
data = data[5:length(data)]
len(data)
length(data)
hist(data)
